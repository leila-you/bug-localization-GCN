{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "حتما! اینجا یک کد کامل پایتون هست که:\n",
    "\n",
    "از فایل JSON ورودی، گراف رو می‌خونه (شامل نودها و یال‌ها)\n",
    "\n",
    "ویژگی‌ها و برچسب‌ها رو از داده‌ها استخراج می‌کنه\n",
    "\n",
    "با GraphSMOTE نمونه‌های جدید تولید می‌کنه\n",
    "\n",
    "نودها و یال‌های اصلی و مصنوعی رو توی یه فایل JSON خروجی می‌نویسه\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph...\n",
      "Classes found: ['bug-free', 'insert-node', 'update']\n",
      "Number of samples per class:\n",
      "  Class 0 (bug-free): 2926 samples\n",
      "  Class 1 (insert-node): 16 samples\n",
      "  Class 2 (update): 2 samples\n",
      "Running GraphSMOTE...\n",
      "Number of synthetic nodes generated: 290\n",
      "Saving new graph with synthetic samples...\n",
      "Wrote 3234 nodes and 8554 edges to C:/Users/Leila/datasetpy/buggy/output_graph_with_synthetic.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class GraphSMOTE:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fit_resample(self, x, y, edge_index):\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        max_count = counts.max()\n",
    "\n",
    "        target_count = int(max_count * 0.1)  # 40% of majority class\n",
    "\n",
    "        x_res = x.copy()\n",
    "        y_res = y.copy()\n",
    "        edge_res = edge_index.copy()\n",
    "\n",
    "        synthetic_samples = []\n",
    "        synthetic_labels = []\n",
    "\n",
    "        for cls, count in zip(unique_classes, counts):\n",
    "            if count >= target_count:\n",
    "                # Enough samples, skip\n",
    "                continue\n",
    "\n",
    "            idx_cls = np.where(y == cls)[0]\n",
    "            x_minority = x[idx_cls]\n",
    "\n",
    "            k_neighbors = min(self.k, len(x_minority) - 1)\n",
    "            if k_neighbors < 1:\n",
    "                print(f\"Warning: minority class {cls} has {len(x_minority)} sample(s), skipping synthetic generation.\")\n",
    "                continue\n",
    "\n",
    "            nn = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(x_minority)\n",
    "            neighbors = nn.kneighbors(x_minority, return_distance=False)\n",
    "\n",
    "            n_synth_needed = target_count - count\n",
    "            n_synth_per_sample = max(1, n_synth_needed // count)\n",
    "\n",
    "            for i in range(len(x_minority)):\n",
    "                for _ in range(n_synth_per_sample):\n",
    "                    nn_idx = np.random.choice(neighbors[i][1:])\n",
    "                    diff = x_minority[nn_idx] - x_minority[i]\n",
    "                    gap = np.random.rand()\n",
    "                    synthetic = x_minority[i] + gap * diff\n",
    "                    synthetic_samples.append(synthetic)\n",
    "                    synthetic_labels.append(cls)\n",
    "                    if len(synthetic_samples) >= n_synth_needed:\n",
    "                        break\n",
    "                if len(synthetic_samples) >= n_synth_needed:\n",
    "                    break\n",
    "\n",
    "        if synthetic_samples:\n",
    "            x_synth = np.vstack(synthetic_samples)\n",
    "            y_synth = np.array(synthetic_labels)\n",
    "            x_new = np.vstack([x_res, x_synth])\n",
    "            y_new = np.hstack([y_res, y_synth])\n",
    "        else:\n",
    "            x_new, y_new = x_res, y_res\n",
    "\n",
    "        return x_new, y_new, edge_res\n",
    "\n",
    "def load_graph_from_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    edges = data[\"edges\"]\n",
    "\n",
    "    # کلاس در اندیس 3\n",
    "    node_classes = [node[3] for node in nodes]\n",
    "    unique_classes = list(sorted(set(node_classes)))\n",
    "    class_to_int = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "    y = np.array([class_to_int[cls] for cls in node_classes])\n",
    "\n",
    "    # ویژگی‌ها از اندیس 1 و 2\n",
    "    def safe_float(val):\n",
    "        try:\n",
    "            return float(val)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    x = np.array([[safe_float(node[1]), safe_float(node[2])] for node in nodes])\n",
    "\n",
    "    edge_index = np.array(edges).T\n",
    "\n",
    "    return x, y, edge_index, nodes, edges, class_to_int\n",
    "\n",
    "def save_graph_with_synthetic(x_orig, y_orig, edge_orig, x_res, y_res, edge_res, nodes_orig, edges_orig, class_to_int, filename):\n",
    "    n_orig = x_orig.shape[0]\n",
    "    n_res = x_res.shape[0]\n",
    "    int_to_class = {v: k for k, v in class_to_int.items()}\n",
    "\n",
    "    nodes_synth = []\n",
    "    for i in range(n_orig, n_res):\n",
    "        idx = i\n",
    "        node_class = int_to_class[y_res[i]]\n",
    "        val1 = str(x_res[i, 0])\n",
    "        val2 = str(x_res[i, 1])\n",
    "        nodes_synth.append([idx, val1, val2, node_class])\n",
    "\n",
    "    all_nodes = nodes_orig + nodes_synth\n",
    "    all_edges = edges_orig  # اگر یال مصنوعی داری باید اضافه کنی اینجا\n",
    "\n",
    "    out_data = {\n",
    "        \"nodes\": all_nodes,\n",
    "        \"edges\": all_edges\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Wrote {len(all_nodes)} nodes and {len(all_edges)} edges to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"C:/Users/Leila/datasetpy/buggy/youtube-dl_4_jsinterp.json\"\n",
    "    output_file = \"C:/Users/Leila/datasetpy/buggy/output_graph_with_synthetic.json\"\n",
    "\n",
    "    print(\"Loading graph...\")\n",
    "    x, y, edge_index, nodes, edges, class_to_int = load_graph_from_json(input_file)\n",
    "\n",
    "    unique_classes = list(class_to_int.keys())\n",
    "    counts = {cls: np.sum(y == idx) for cls, idx in class_to_int.items()}\n",
    "\n",
    "    print(\"Classes found:\", unique_classes)\n",
    "    print(\"Number of samples per class:\")\n",
    "    for cls in unique_classes:\n",
    "        print(f\"  Class {class_to_int[cls]} ({cls}): {counts[cls]} samples\")\n",
    "\n",
    "    print(\"Running GraphSMOTE...\")\n",
    "    smote = GraphSMOTE(k=5)\n",
    "    x_res, y_res, edge_res = smote.fit_resample(x, y, edge_index)\n",
    "\n",
    "    n_synth = x_res.shape[0] - x.shape[0]\n",
    "    print(f\"Number of synthetic nodes generated: {n_synth}\")\n",
    "\n",
    "    print(\"Saving new graph with synthetic samples...\")\n",
    "    save_graph_with_synthetic(x, y, edge_index, x_res, y_res, edge_res, nodes, edges, class_to_int, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph...\n",
      "Classes found: ['bug-free', 'insert-node', 'update']\n",
      "Number of samples per class:\n",
      "  Class 0 (bug-free): 2926 samples\n",
      "  Class 1 (insert-node): 16 samples\n",
      "  Class 2 (update): 2 samples\n",
      "Running GraphSMOTE...\n",
      "Number of synthetic nodes generated: 875\n",
      "Saving new graph with synthetic samples...\n",
      "Wrote 3819 nodes and 8554 edges to C:/Users/Leila/datasetpy/buggy/output_graph_with_synthetic.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class GraphSMOTE:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fit_resample(self, x, y, edge_index):\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        max_count = counts.max()\n",
    "\n",
    "        target_count = int(max_count * 0.3)  # 40% of majority class\n",
    "\n",
    "        x_res = x.copy()\n",
    "        y_res = y.copy()\n",
    "        edge_res = edge_index.copy()\n",
    "\n",
    "        synthetic_samples = []\n",
    "        synthetic_labels = []\n",
    "\n",
    "        for cls, count in zip(unique_classes, counts):\n",
    "            if count >= target_count:\n",
    "                # Enough samples, skip\n",
    "                continue\n",
    "\n",
    "            idx_cls = np.where(y == cls)[0]\n",
    "            x_minority = x[idx_cls]\n",
    "\n",
    "            k_neighbors = min(self.k, len(x_minority) - 1)\n",
    "            if k_neighbors < 1:\n",
    "                print(f\"Warning: minority class {cls} has {len(x_minority)} sample(s), skipping synthetic generation.\")\n",
    "                continue\n",
    "\n",
    "            nn = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(x_minority)\n",
    "            neighbors = nn.kneighbors(x_minority, return_distance=False)\n",
    "\n",
    "            n_synth_needed = target_count - count\n",
    "            n_synth_per_sample = max(1, n_synth_needed // count)\n",
    "\n",
    "            for i in range(len(x_minority)):\n",
    "                for _ in range(n_synth_per_sample):\n",
    "                    nn_idx = np.random.choice(neighbors[i][1:])\n",
    "                    diff = x_minority[nn_idx] - x_minority[i]\n",
    "                    gap = np.random.rand()\n",
    "                    synthetic = x_minority[i] + gap * diff\n",
    "                    synthetic_samples.append(synthetic)\n",
    "                    synthetic_labels.append(cls)\n",
    "                    if len(synthetic_samples) >= n_synth_needed:\n",
    "                        break\n",
    "                if len(synthetic_samples) >= n_synth_needed:\n",
    "                    break\n",
    "\n",
    "        if synthetic_samples:\n",
    "            x_synth = np.vstack(synthetic_samples)\n",
    "            y_synth = np.array(synthetic_labels)\n",
    "            x_new = np.vstack([x_res, x_synth])\n",
    "            y_new = np.hstack([y_res, y_synth])\n",
    "        else:\n",
    "            x_new, y_new = x_res, y_res\n",
    "\n",
    "        return x_new, y_new, edge_res\n",
    "\n",
    "def load_graph_from_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    edges = data[\"edges\"]\n",
    "\n",
    "    # استخراج نگاشت type و value از داده‌های متنی\n",
    "    type_map = {}\n",
    "    value_map = {}\n",
    "\n",
    "    def encode_categorical(val, mapping):\n",
    "        if val not in mapping:\n",
    "            mapping[val] = len(mapping)\n",
    "        return mapping[val]\n",
    "\n",
    "    # کلاس‌ها را استخراج و نگاشت کن\n",
    "    node_classes = [node[3] for node in nodes]\n",
    "    unique_classes = list(sorted(set(node_classes)))\n",
    "    class_to_int = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "    y = np.array([class_to_int[cls] for cls in node_classes])\n",
    "\n",
    "    # ویژگی‌های type و value را به عدد نگاشت کن\n",
    "    x = []\n",
    "    for node in nodes:\n",
    "        type_feat = encode_categorical(node[1], type_map)\n",
    "        val_feat = encode_categorical(node[2], value_map)\n",
    "        x.append([type_feat, val_feat])\n",
    "    x = np.array(x)\n",
    "\n",
    "    edge_index = np.array(edges).T\n",
    "\n",
    "    return x, y, edge_index, nodes, edges, class_to_int, type_map, value_map\n",
    "\n",
    "def save_graph_with_synthetic(x_orig, y_orig, edge_orig, x_res, y_res, edge_res, nodes_orig, edges_orig, class_to_int, type_map, value_map, filename):\n",
    "    n_orig = x_orig.shape[0]\n",
    "    n_res = x_res.shape[0]\n",
    "\n",
    "    int_to_class = {v: k for k, v in class_to_int.items()}\n",
    "    inv_type_map = {v: k for k, v in type_map.items()}\n",
    "    inv_value_map = {v: k for k, v in value_map.items()}\n",
    "\n",
    "    nodes_synth = []\n",
    "    for i in range(n_orig, n_res):\n",
    "        node_class = int_to_class[y_res[i]]\n",
    "\n",
    "        synthetic_type_num = int(round(x_res[i, 0]))\n",
    "        synthetic_value_num = int(round(x_res[i, 1]))\n",
    "\n",
    "        synthetic_type_str = inv_type_map.get(synthetic_type_num, \"Unknown\")\n",
    "        synthetic_value_str = inv_value_map.get(synthetic_value_num, \"Unknown\")\n",
    "\n",
    "        nodes_synth.append([i, synthetic_type_str, synthetic_value_str, node_class])\n",
    "\n",
    "    all_nodes = nodes_orig + nodes_synth\n",
    "    all_edges = edges_orig  # اگر یال مصنوعی داری اضافه کن\n",
    "\n",
    "    out_data = {\n",
    "        \"nodes\": all_nodes,\n",
    "        \"edges\": all_edges\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Wrote {len(all_nodes)} nodes and {len(all_edges)} edges to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"C:/Users/Leila/datasetpy/buggy/youtube-dl_4_jsinterp.json\"\n",
    "    output_file = \"C:/Users/Leila/datasetpy/buggy/output_graph_with_synthetic.json\"\n",
    "\n",
    "    print(\"Loading graph...\")\n",
    "    x, y, edge_index, nodes, edges, class_to_int, type_map, value_map = load_graph_from_json(input_file)\n",
    "\n",
    "    unique_classes = list(class_to_int.keys())\n",
    "    counts = {cls: np.sum(y == idx) for cls, idx in class_to_int.items()}\n",
    "\n",
    "    print(\"Classes found:\", unique_classes)\n",
    "    print(\"Number of samples per class:\")\n",
    "    for cls in unique_classes:\n",
    "        print(f\"  Class {class_to_int[cls]} ({cls}): {counts[cls]} samples\")\n",
    "\n",
    "    print(\"Running GraphSMOTE...\")\n",
    "    smote = GraphSMOTE(k=5)\n",
    "    x_res, y_res, edge_res = smote.fit_resample(x, y, edge_index)\n",
    "\n",
    "    n_synth = x_res.shape[0] - x.shape[0]\n",
    "    print(f\"Number of synthetic nodes generated: {n_synth}\")\n",
    "\n",
    "    print(\"Saving new graph with synthetic samples...\")\n",
    "    save_graph_with_synthetic(x, y, edge_index, x_res, y_res, edge_res, nodes, edges, class_to_int, type_map, value_map, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
